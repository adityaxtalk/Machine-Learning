{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Ea3krURvLS",
        "colab_type": "text"
      },
      "source": [
        "###Importing library and prerequisite tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mrn6QSXRi8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "9bf31268-e5fe-4b7c-e706-e91986b1fe3d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Key0s__t4vYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re,string,random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPwsID-L8czK",
        "colab_type": "text"
      },
      "source": [
        "##Preprocessing functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciYXgqFW80gP",
        "colab_type": "text"
      },
      "source": [
        " *Remove* *noise*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVWxHH-VWuc7",
        "colab_type": "text"
      },
      "source": [
        "Noise is any part of the text that doesn't add meaning or information to data.\n",
        "In twitter dataset, with the help of regex we will search for and remove these items:\n",
        "1. Hyperlinks\n",
        "2. Twitter handles\n",
        "3. Punctuation and special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNJT_r88l6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens,stop_words=()):\n",
        "  tokens=[]\n",
        "  for token,tag in pos_tag(tweet_tokens):\n",
        "    token=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',token)\n",
        "    token=re.sub(\"(@[A-Za-z0-9_]+)\",\"\",token)\n",
        "    if tag.startswith(\"NN\"):\n",
        "      pos='n'\n",
        "    elif tag.startswith(\"VB\"):\n",
        "      pos='v'\n",
        "    else:\n",
        "      pos='a'\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    token=lemmatizer.lemmatize(token,pos)\n",
        "    if len(token)>0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "      tokens.append(token.lower())\n",
        "  return tokens\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTycpyUBCmmm",
        "colab_type": "text"
      },
      "source": [
        "Determining word density"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI0zbrbSCmOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_words(tokens_list):\n",
        "  for tokens in tokens_list:\n",
        "    for token in tokens:\n",
        "      yield token"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1hN9cY1DgPX",
        "colab_type": "text"
      },
      "source": [
        "Converting Tokens to a dictionary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1__ae09Dgsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets_for_model(tokens_list):\n",
        "   for tweet_tokens in tokens_list:\n",
        "      yield dict([token,True] for token in tweet_tokens)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YADesp1GJWGv",
        "colab_type": "text"
      },
      "source": [
        "###Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3euWwDJaOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
        "text=twitter_samples.strings('tweets.20150430-223406.json')\n",
        "tweet_tokens=twitter_samples.tokenized('positive_tweets.json')[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmUhlG_KiPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=stopwords.words('english')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jkfJnUK7C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweets_tokens= twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweets_tokens= twitter_samples.tokenized('negative_tweets.json')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrkQI3OGLN-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_cleaned_tokens_list=[]\n",
        "negative_cleaned_tokens_list=[]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABxfGYrwL2il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXTL_v0lLXaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tokens in positive_tweets_tokens:\n",
        "  positive_cleaned_tokens_list.append(remove_noise(tokens,stop_words))\n",
        "for tokens in negative_tweets_tokens:\n",
        "  negative_cleaned_tokens_list.append(remove_noise(tokens,stop_words))  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-rKYj3dNmm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d7869b85-4758-409f-ca1e-33b85f362003"
      },
      "source": [
        "all_pos_words=get_all_words(positive_cleaned_tokens_list)\n",
        "freq_dist_pos=FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG_kkpCDOMbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tokens_for_model=get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model=get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul9s4mfJO1A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_dataset=[(tweet_dict,\"Positive\") for tweet_dict in positive_tokens_for_model]\n",
        "negative_dataset=[(tweet_dict,\"Negative\") for tweet_dict in negative_tokens_for_model]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIbjuugiPNQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=positive_dataset+negative_dataset"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx3wU0yoPXcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(dataset)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKDSNlsPeRq",
        "colab_type": "text"
      },
      "source": [
        "###Split dataset into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TySTkWekPdYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=dataset[:7000]\n",
        "test_data=dataset[7000:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JlNOyG3Pw8L",
        "colab_type": "text"
      },
      "source": [
        "###Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahYExqzmPwfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier=NaiveBayesClassifier.train(train_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdjqQfwtQA4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f51b5f64-8253-4905-9276-9acbf4dec567"
      },
      "source": [
        "print(\"Accuracy is: {:.2f}\".format(classify.accuracy(classifier,test_data)*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 99.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p71LCcbsTn-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "2441c77d-5531-4f99-fa52-021d362e4fe0"
      },
      "source": [
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                      :) = True           Positi : Negati =   1649.7 : 1.0\n",
            "                follower = True           Positi : Negati =     37.7 : 1.0\n",
            "                  arrive = True           Positi : Negati =     31.7 : 1.0\n",
            "                     sad = True           Negati : Positi =     23.3 : 1.0\n",
            "                     bam = True           Positi : Negati =     21.7 : 1.0\n",
            "                    glad = True           Positi : Negati =     21.7 : 1.0\n",
            "                     x15 = True           Negati : Positi =     16.3 : 1.0\n",
            "                 welcome = True           Positi : Negati =     16.1 : 1.0\n",
            "              appreciate = True           Positi : Negati =     15.7 : 1.0\n",
            "                    damn = True           Negati : Positi =     15.7 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrQfsFlUIeo",
        "colab_type": "text"
      },
      "source": [
        "###Validating on custom tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyN8ZtcUEYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "059d748d-166d-432b-9946-671e0cec8dd0"
      },
      "source": [
        "custom_tweet=\"I ordered from Zomato, they screwed up, never used the app again.\"\n",
        "custom_tokens=remove_noise(word_tokenize(custom_tweet))\n",
        "print(custom_tweet,classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I ordered from Zomato, they screwed up, never used the app again. Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}